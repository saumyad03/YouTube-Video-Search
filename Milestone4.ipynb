{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af1a47d",
   "metadata": {},
   "source": [
    "For this assignment, we followed the following tutorial: https://keras.io/examples/vision/nl_image_search/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d00738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb01b23",
   "metadata": {},
   "source": [
    "First, we downloaded and extracted the dataset, placing it into our working directory. There is a folder titled images containing JPEGs of all the image training data. Additionally, there is a captions.txt that contains comma-separated values on each line, the first being the image name and the second being a caption for that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd0c0915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40455 captions.\n"
     ]
    }
   ],
   "source": [
    "# maps image path to list of all its captions\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "# populates this dictionary by iterating through captions.txt\n",
    "inFile = open(\"captions.txt\")\n",
    "count = 0\n",
    "for line in inFile:\n",
    "    count += 1\n",
    "    if count == 1: continue\n",
    "    fields = line.split(\",\")\n",
    "    fileName = fields[0]\n",
    "    caption = \",\".join(fields[1:])\n",
    "    image_path_to_caption[\"Images/\" + fileName].append(caption)\n",
    "print(\"There are \" + str(count - 1) + \" captions.\") # excludes headers\n",
    "inFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bd92b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8091 images.\n"
     ]
    }
   ],
   "source": [
    "# gets list of all image paths\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "print(\"There are \" + str(len(image_paths)) + \" images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "526ace64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image count used for training and how many captions used per image\n",
    "train_size = int(len(image_paths) * 0.4)\n",
    "captions_per_image = 2\n",
    "# image count for testing\n",
    "valid_size = int(len(image_paths) * 0.1)\n",
    "# how many images stored in each tfrecords file\n",
    "images_per_file = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33e8a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6472 training examples were written to tfrecord files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1618 evaluation examples were written to tfrecord files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# slices image paths list to retrieve for training and testing\n",
    "# calculates how many files are needed to store the images\n",
    "# creates directories to hold training and testing data respectively\n",
    "tfrecords_dir = \"tfrecords\"\n",
    "\n",
    "train_image_paths = image_paths[:train_size]\n",
    "num_train_files = int(np.ceil(train_size / images_per_file))\n",
    "train_files_prefix = os.path.join(tfrecords_dir, \"train\")\n",
    "\n",
    "valid_image_paths = image_paths[-valid_size:]\n",
    "num_valid_files = int(np.ceil(valid_size / images_per_file))\n",
    "valid_files_prefix = os.path.join(tfrecords_dir, \"valid\")\n",
    "\n",
    "tf.io.gfile.makedirs(tfrecords_dir)\n",
    "\n",
    "# creates Feature object that can be stored to tfrecord file\n",
    "# from the byte string passed into function\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "# creates Example object with features caption and raw_image\n",
    "# the caption and raw image are converted to byte strings which are\n",
    "# converted to Feature objects\n",
    "def create_example(image_path, caption):\n",
    "    feature = {\n",
    "        \"caption\": bytes_feature(caption.encode()),\n",
    "        \"raw_image\": bytes_feature(tf.io.read_file(image_path).numpy()),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "# writes examples to tfrecords files\n",
    "def write_tfrecords(file_name, image_paths):\n",
    "    # stores captions and image paths\n",
    "    caption_list = []\n",
    "    image_path_list = []\n",
    "    example_idx = 0\n",
    "    # iterates through image paths, retrieving subset of\n",
    "    # captions for each image and appending image path\n",
    "    # and caption to their respective lists\n",
    "    for image_path in image_paths:\n",
    "        captions = image_path_to_caption[image_path][:captions_per_image]\n",
    "        caption_list.extend(captions)\n",
    "        image_path_list.extend([image_path] * len(captions))\n",
    "    # writes created examples to tfrecords file, returning\n",
    "    # number of examples written\n",
    "    with tf.io.TFRecordWriter(file_name) as writer:\n",
    "        for example_idx in range(len(image_path_list)):\n",
    "            example = create_example(\n",
    "                image_path_list[example_idx], caption_list[example_idx]\n",
    "            )\n",
    "            writer.write(example.SerializeToString())\n",
    "    return example_idx + 1\n",
    "\n",
    "# writes examples to the appropriate number of tfrecords files\n",
    "# returns number of examples written\n",
    "def write_data(image_paths, num_files, files_prefix):\n",
    "    example_counter = 0\n",
    "    for file_idx in tqdm(range(num_files)):\n",
    "        file_name = files_prefix + \"-%02d.tfrecord\" % (file_idx)\n",
    "        start_idx = images_per_file * file_idx\n",
    "        end_idx = start_idx + images_per_file\n",
    "        example_counter += write_tfrecords(file_name, image_paths[start_idx:end_idx])\n",
    "    return example_counter\n",
    "\n",
    "# writes training data to tf record files\n",
    "train_example_count = write_data(train_image_paths, num_train_files, train_files_prefix)\n",
    "print(f\"{train_example_count} training examples were written to tfrecord files.\")\n",
    "\n",
    "# writes testing data to td record files\n",
    "valid_example_count = write_data(valid_image_paths, num_valid_files, valid_files_prefix)\n",
    "print(f\"{valid_example_count} evaluation examples were written to tfrecord files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38cb584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines dictionary that describes expected features\n",
    "# in Example objects read from tfrecord file\n",
    "feature_description = {\n",
    "    \"caption\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"raw_image\": tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "# parses the features of a serialized tfrecord Example object\n",
    "def read_example(example):\n",
    "    features = tf.io.parse_single_example(example, feature_description)\n",
    "    raw_image = features.pop(\"raw_image\")\n",
    "    # decodes image into image tensor\n",
    "    features[\"image\"] = tf.image.resize(\n",
    "        tf.image.decode_jpeg(raw_image, channels=3), size=(299, 299)\n",
    "    )\n",
    "    return features\n",
    "\n",
    "#  returns TFRecordDataset object by converting serialized data\n",
    "# to normal data for each file that matches a certain pattern\n",
    "def get_dataset(file_pattern, batch_size):\n",
    "    return (\n",
    "        tf.data.TFRecordDataset(tf.data.Dataset.list_files(file_pattern))\n",
    "        .map(\n",
    "            read_example,\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=False,\n",
    "        )\n",
    "        .shuffle(batch_size * 10)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "107725f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms image and text embeddings to same embedding space \n",
    "# with same dimensionality\n",
    "def project_embeddings(embeddings, num_projection_layers, projection_dims, dropout_rate):\n",
    "    projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n",
    "    for _ in range(num_projection_layers):\n",
    "        x = tf.nn.gelu(projected_embeddings)\n",
    "        x = layers.Dense(projection_dims)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Add()([projected_embeddings, x])\n",
    "        projected_embeddings = layers.LayerNormalization()(x)\n",
    "    return projected_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f809c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation for the image encoder\n",
    "def create_vision_encoder(num_projection_layers, projection_dims, dropout_rate, trainable=False):\n",
    "    # Loads pre-trained Xception model used as base encoder\n",
    "    xception = keras.applications.Xception(include_top=False, weights=\"imagenet\", pooling=\"avg\")\n",
    "    # Sets trainability of the base encoder\n",
    "    for layer in xception.layers:\n",
    "        layer.trainable = trainable\n",
    "    # Takes images as inputs\n",
    "    inputs = layers.Input(shape=(299, 299, 3), name=\"image_input\")\n",
    "    # Preprocesses input image\n",
    "    xception_input = tf.keras.applications.xception.preprocess_input(inputs)\n",
    "    # Generate embeddings\n",
    "    embeddings = xception(xception_input)\n",
    "    # Projects embeddings produced by model\n",
    "    outputs = project_embeddings(embeddings, num_projection_layers, projection_dims, dropout_rate)\n",
    "    # Creates the image encoder model\n",
    "    return keras.Model(inputs, outputs, name=\"vision_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3144ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation for the caption encoder\n",
    "def create_text_encoder(num_projection_layers, projection_dims, dropout_rate, trainable=False):\n",
    "    # Loads BERT preprocessing module\n",
    "    preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\", name=\"text_preprocessing\")\n",
    "    # Load pre-trained BERT model used as the base encoder\n",
    "    bert = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\", trainable=trainable)\n",
    "    # Sets trainability of the base encoder.\n",
    "    bert.trainable = trainable\n",
    "    # Takes text as input\n",
    "    inputs = layers.Input(shape=(), dtype=tf.string, name=\"text_input\")\n",
    "    # Preprocess text\n",
    "    bert_inputs = preprocess(inputs)\n",
    "    # Generates embeddings\n",
    "    embeddings = bert(bert_inputs)[\"pooled_output\"]\n",
    "    # Projects embeddings produced by model.\n",
    "    outputs = project_embeddings(embeddings, num_projection_layers, projection_dims, dropout_rate)\n",
    "    # Creates the caption encoder model\n",
    "    return keras.Model(inputs, outputs, name=\"text_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c875dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of dual encoder\n",
    "class DualEncoder(keras.Model):\n",
    "    # constructor - initializes attributes of dual encoder\n",
    "    def __init__(self, text_encoder, image_encoder, temperature=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.temperature = temperature\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "    # return loss tracker metrics\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "    \n",
    "    # forward pass\n",
    "    def call(self, features, training=False):\n",
    "        # Places each encoder on a separate GPU (if available)\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            # Gets the embeddings for the captions.\n",
    "            caption_embeddings = text_encoder(features[\"caption\"], training=training)\n",
    "        with tf.device(\"/gpu:1\"):\n",
    "            # Gets the embeddings for the images\n",
    "            image_embeddings = vision_encoder(features[\"image\"], training=training)\n",
    "        return caption_embeddings, image_embeddings\n",
    "    \n",
    "    # computes the loss by computing pairwise dot-product similarity\n",
    "    # and average them to get the target similarity\n",
    "    # finally, uses cross entropy to computer loss between\n",
    "    # targets and predictions\n",
    "    def compute_loss(self, caption_embeddings, image_embeddings):\n",
    "        # logits[i][j] is the dot_similarity(caption_i, image_j).\n",
    "        logits = (\n",
    "            tf.matmul(caption_embeddings, image_embeddings, transpose_b=True)\n",
    "            / self.temperature\n",
    "        )\n",
    "        # images_similarity[i][j] is the dot_similarity(image_i, image_j).\n",
    "        images_similarity = tf.matmul(\n",
    "            image_embeddings, image_embeddings, transpose_b=True\n",
    "        )\n",
    "        # captions_similarity[i][j] is the dot_similarity(caption_i, caption_j).\n",
    "        captions_similarity = tf.matmul(\n",
    "            caption_embeddings, caption_embeddings, transpose_b=True\n",
    "        )\n",
    "        # targets[i][j] = avarage dot_similarity(caption_i, caption_j) and dot_similarity(image_i, image_j).\n",
    "        targets = keras.activations.softmax(\n",
    "            (captions_similarity + images_similarity) / (2 * self.temperature)\n",
    "        )\n",
    "        # computes the loss for the captions using crossentropy\n",
    "        captions_loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=targets, y_pred=logits, from_logits=True\n",
    "        )\n",
    "        # computes the loss for the images using crossentropy\n",
    "        images_loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True\n",
    "        )\n",
    "        # Return the mean of the loss over the batch.\n",
    "        return (captions_loss + images_loss) / 2\n",
    "\n",
    "    # does one training step - allows us to track loss wrt. epochs\n",
    "    def train_step(self, features):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # forward pass\n",
    "            caption_embeddings, image_embeddings = self(features, training=True)\n",
    "            loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        # backward pass\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # monitors loss\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "    # does one testing step - also allows us to track loss wrt. epochs\n",
    "    def test_step(self, features):\n",
    "        caption_embeddings, image_embeddings = self(features, training=False)\n",
    "        loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2699916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets up dual encoder for training\n",
    "num_epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "vision_encoder = create_vision_encoder(\n",
    "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
    ")\n",
    "text_encoder = create_text_encoder(\n",
    "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
    ")\n",
    "dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=0.05)\n",
    "dual_encoder.compile(\n",
    "    optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0fc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 0\n",
      "Number of examples (caption-image pairs): 6472\n",
      "Batch size: 256\n",
      "Steps per epoch: 26\n",
      "Epoch 1/5\n",
      "26/26 [==============================] - 840s 32s/step - loss: 249.4965 - val_loss: 96.5461 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "26/26 [==============================] - 792s 30s/step - loss: 104.2450 - val_loss: 50.0177 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "26/26 [==============================] - 782s 30s/step - loss: 69.1164 - val_loss: 35.1053 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "26/26 [==============================] - 793s 31s/step - loss: 49.6003 - val_loss: 27.3336 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "26/26 [==============================] - ETA: 0s - loss: 39.7696 "
     ]
    }
   ],
   "source": [
    "print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "print(f\"Number of examples (caption-image pairs): {train_example_count}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Steps per epoch: {int(np.ceil(train_example_count / batch_size))}\")\n",
    "train_dataset = get_dataset(os.path.join(tfrecords_dir, \"train-*.tfrecord\"), batch_size)\n",
    "valid_dataset = get_dataset(os.path.join(tfrecords_dir, \"valid-*.tfrecord\"), batch_size)\n",
    "# creates learning rate scheduler callback.\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2, patience=3\n",
    ")\n",
    "# creates early stopping callback.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "history = dual_encoder.fit(\n",
    "    train_dataset,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    ")\n",
    "print(\"Training completed. Saving vision and text encoders...\")\n",
    "vision_encoder.save(\"vision_encoder\")\n",
    "text_encoder.save(\"text_encoder\")\n",
    "print(\"Models are saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ee235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots loss as a function of epochs\n",
    "# for training and testing\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"train\", \"valid\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
